{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11597575,"sourceType":"datasetVersion","datasetId":7273047},{"sourceId":11602270,"sourceType":"datasetVersion","datasetId":7276693},{"sourceId":11612494,"sourceType":"datasetVersion","datasetId":7284014},{"sourceId":11612661,"sourceType":"datasetVersion","datasetId":7284147},{"sourceId":11644762,"sourceType":"datasetVersion","datasetId":7307139}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pickle\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Load the voxel data\nwith open(\"/kaggle/input/voxels/voxel/pdb111d_voxel.pkl\", \"rb\") as f:\n    voxel_data = pickle.load(f)\n\n# Get non-zero voxel positions\nvoxel_presence = np.sum(voxel_data, axis=-1) > 0\nx, y, z = np.where(voxel_presence)\n\n# Create 3D scatter plot\nfig = go.Figure(data=go.Scatter3d(\n    x=x, y=y, z=z,\n    mode='markers',\n    marker=dict(\n        size=2,\n        color=z,\n        colorscale='Viridis',\n        opacity=0.8\n    )\n))\n\nfig.update_layout(\n    scene=dict(\n        xaxis_title='X',\n        yaxis_title='Y',\n        zaxis_title='Z'\n    ),\n    title='3D Voxel Visualization (Non-zero Voxels)',\n    margin=dict(l=0, r=0, b=0, t=30)\n)\n\nfig.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport argparse\nimport time\nfrom google.colab import drive\n\n# --- CONFIGURATION ---\nparser = argparse.ArgumentParser(description=\"Enhanced Feature Extraction for Protein Structures\")\nparser.add_argument(\"--csv_file\", default=\"pdb_voxel_mapping.csv\", type=str, help=\"CSV file mapping voxels to descriptions\")\nparser.add_argument(\"--voxel_folder\", default=\"voxel/voxel\", type=str, help=\"Folder containing voxel grid files\")\nparser.add_argument(\"--output_folder\", default=\"./extracted_features/\", type=str, help=\"Output folder for extracted features\")\nparser.add_argument(\"--batch_size\", default=8, type=int, help=\"Batch size for training and feature extraction\")\nparser.add_argument(\"--num_workers\", default=2, type=int, help=\"Number of workers for data loading\")\nparser.add_argument(\"--pretrain\", action=\"store_true\", help=\"Pretrain the model on a classification task\")\nparser.add_argument(\"--pretrain_epochs\", default=5, type=int, help=\"Number of epochs for pretraining\")\nparser.add_argument(\"--feature_dim\", default=512, type=int, help=\"Dimension of extracted features\")\nparser.add_argument(\"--save_model\", default=\"model_checkpoint.pth\", type=str, help=\"Path to save model checkpoint\")\nparser.add_argument(\"--run_all\", action=\"store_true\", help=\"Run all architectures\", default=True)\n\nargs = parser.parse_args([])  # Empty list for Google Colab\n\n# Check if running in Google Colab\ntry:\n    import google.colab\n    IN_COLAB = True\n    print(\"Running in Google Colab environment\")\n    # Mount Google Drive if not already mounted\n    if not os.path.exists('/content/drive'):\n        drive.mount('/content/drive')\nexcept:\n    IN_COLAB = False\n    print(\"Not running in Google Colab environment\")\n\n# Define configurations for each architecture\nARCHITECTURES = [\n    {\"name\": \"resnet18\", \"use_attention\": False},\n    {\"name\": \"resnet18\", \"use_attention\": True},\n    {\"name\": \"resnet34\", \"use_attention\": False},\n    {\"name\": \"resnet34\", \"use_attention\": True},\n    {\"name\": \"resnet50\", \"use_attention\": False},\n    {\"name\": \"vit\", \"use_attention\": False}  # ViT doesn't use conventional attention flag\n]\n\n# Make sure output folder exists\nbase_output_folder = args.output_folder\nos.makedirs(base_output_folder, exist_ok=True)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- DATASET ---\nclass VoxelDataset(Dataset):\n    def __init__(self, csv_file, voxel_folder, classification=False):\n        \"\"\"\n        Args:\n            csv_file (str): Path to the csv file with voxel file paths and descriptions\n            voxel_folder (str): Directory with all the voxel files\n            classification (bool): If True, prepare data for classification task\n        \"\"\"\n        self.df = pd.read_csv(csv_file)\n        self.voxel_folder = voxel_folder\n        self.classification = classification\n\n        # For classification pretraining, we'll create pseudo-labels based on structural features\n        if classification:\n            # This is a simplification - in reality, you might want to cluster proteins or use real labels\n            # Here we're just using position in the dataset as a proxy for classes (for demonstration)\n            num_classes = min(len(self.df), 10)  # Limit to 10 classes for simplicity\n            self.df['class_label'] = self.df.index % num_classes\n\n        # Determine voxel file column\n        if \"Voxel File\" in self.df.columns:\n            self.voxel_column = \"Voxel File\"\n        elif \"voxel_file\" in self.df.columns:\n            self.voxel_column = \"voxel_file\"\n        else:\n            self.voxel_column = self.df.columns[0]  # Assume first column contains voxel file paths\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        voxel_file = row[self.voxel_column]\n        voxel_path = os.path.join(self.voxel_folder, voxel_file)\n\n        if not os.path.exists(voxel_path):\n            print(f\"⚠️ Warning: File not found - {voxel_path}\")\n            voxel_grid = np.zeros((1, 64, 64, 64), dtype=np.float32)\n        else:\n            try:\n                with open(voxel_path, \"rb\") as f:\n                    voxel_grid = pickle.load(f)\n                voxel_grid = np.array(voxel_grid, dtype=np.float32)\n\n                # Handle different voxel formats\n                if voxel_grid.ndim == 4:\n                    # Some voxels might have channels dimension\n                    voxel_grid = voxel_grid[..., 0]\n\n                # Add channel dimension if needed\n                if voxel_grid.ndim == 3:\n                    voxel_grid = np.expand_dims(voxel_grid, axis=0)  # (1, 64, 64, 64)\n            except Exception as e:\n                print(f\"❌ Error loading {voxel_path}: {e}\")\n                voxel_grid = np.zeros((1, 64, 64, 64), dtype=np.float32)\n\n        # Convert to tensor\n        voxel_tensor = torch.tensor(voxel_grid, dtype=torch.float32)\n\n        if self.classification:\n            class_label = row['class_label']\n            return voxel_tensor, class_label, voxel_file\n        else:\n            return voxel_tensor, voxel_file\n\n\n# --- ATTENTION MODULE ---\nclass SpatialAttention3D(nn.Module):\n    def __init__(self, in_channels):\n        super(SpatialAttention3D, self).__init__()\n        self.conv1 = nn.Conv3d(in_channels, in_channels // 8, kernel_size=1)\n        self.conv2 = nn.Conv3d(in_channels // 8, 1, kernel_size=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        # Channel attention first\n        attention = F.avg_pool3d(x, x.size()[2:])  # Global average pooling\n        attention = self.conv1(x)\n        attention = F.relu(attention)\n        attention = self.conv2(attention)\n        attention_map = self.sigmoid(attention)\n\n        # Apply attention\n        return x * attention_map\n\n\n# --- RESIDUAL BLOCK ---\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, use_attention=False):\n        super(ResidualBlock, self).__init__()\n        self.use_attention = use_attention\n\n        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm3d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm3d(out_channels)\n\n        if self.use_attention:\n            self.attention = SpatialAttention3D(out_channels)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm3d(out_channels)\n            )\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.use_attention:\n            out = self.attention(out)\n\n        out += self.shortcut(residual)\n        out = self.relu(out)\n\n        return out\n\n\n# --- RESNET MODEL ---\nclass ResNet3D(nn.Module):\n    def __init__(self, block_counts, input_shape=(1, 64, 64, 64), feature_dim=512, num_classes=10, use_attention=False):\n        super(ResNet3D, self).__init__()\n        self.in_channels = 64\n        self.use_attention = use_attention\n        self.feature_dim = feature_dim\n\n        # Initial convolution\n        self.conv1 = nn.Conv3d(input_shape[0], 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm3d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n\n        # Residual layers\n        self.layer1 = self._make_layer(64, block_counts[0], stride=1)\n        self.layer2 = self._make_layer(128, block_counts[1], stride=2)\n        self.layer3 = self._make_layer(256, block_counts[2], stride=2)\n        self.layer4 = self._make_layer(512, block_counts[3], stride=2)\n\n        # Global average pooling and final FC layer\n        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.fc_features = nn.Linear(512, feature_dim)\n        self.fc_classifier = nn.Linear(feature_dim, num_classes)\n\n    def _make_layer(self, out_channels, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(ResidualBlock(self.in_channels, out_channels, stride, self.use_attention))\n            self.in_channels = out_channels\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # Feature extraction\n        features = self.forward_features(x)\n\n        # Classification head\n        x = self.fc_classifier(features)\n        return x, features\n\n    def forward_features(self, x):\n        # Feature extraction without final classification\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_features(x)\n        return x\n\n\n# --- VISION TRANSFORMER FOR 3D ---\nclass PatchEmbed3D(nn.Module):\n    \"\"\"Convert 3D volume into patches and embed them\"\"\"\n    def __init__(self, vol_size=64, patch_size=8, in_chans=1, embed_dim=768):\n        super().__init__()\n        self.vol_size = vol_size\n        self.patch_size = patch_size\n        num_patches = (vol_size // patch_size) ** 3\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv3d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W, D = x.shape\n        assert H == W == D == self.vol_size, \"Input volume size doesn't match\"\n\n        # (B, C, H, W, D) -> (B, embed_dim, H/patch_size, W/patch_size, D/patch_size)\n        x = self.proj(x)\n        # Reshape to (B, num_patches, embed_dim)\n        x = x.flatten(2).transpose(1, 2)\n        return x\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-head self-attention\"\"\"\n    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass MLP(nn.Module):\n    \"\"\"MLP block in Transformer\"\"\"\n    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0.):\n        super().__init__()\n        out_features = out_features or in_features\n        hidden_features = hidden_features or in_features\n        self.fc1 = nn.Linear(in_features, hidden_features)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_features, out_features)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"Transformer block with attention and MLP\"\"\"\n    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n        self.norm2 = nn.LayerNorm(dim)\n        mlp_hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass VisionTransformer3D(nn.Module):\n    \"\"\"Vision Transformer for 3D volumes\"\"\"\n    def __init__(self, vol_size=64, patch_size=8, in_chans=1, num_classes=10, embed_dim=768,\n                 depth=12, num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0.1, feature_dim=512):\n        super().__init__()\n        self.patch_embed = PatchEmbed3D(vol_size=vol_size, patch_size=patch_size,\n                                      in_chans=in_chans, embed_dim=embed_dim)\n        num_patches = self.patch_embed.num_patches\n\n        # Add cls token\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        # Position embeddings\n        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n        self.pos_drop = nn.Dropout(p=drop_rate)\n\n        # Transformer blocks\n        self.blocks = nn.ModuleList([\n            TransformerBlock(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n                           qkv_bias=qkv_bias, drop=drop_rate, attn_drop=drop_rate)\n            for i in range(depth)\n        ])\n\n        self.norm = nn.LayerNorm(embed_dim)\n\n        # Features and classification heads\n        self.fc_features = nn.Linear(embed_dim, feature_dim)\n        self.fc_classifier = nn.Linear(feature_dim, num_classes)\n\n        # Initialize weights\n        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n        nn.init.trunc_normal_(self.cls_token, std=0.02)\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Linear):\n            nn.init.trunc_normal_(m.weight, std=0.02)\n            if m.bias is not None:\n                nn.init.constant_(m.bias, 0)\n        elif isinstance(m, nn.LayerNorm):\n            nn.init.constant_(m.bias, 0)\n            nn.init.constant_(m.weight, 1.0)\n\n    def forward_features(self, x):\n        # Patch embedding\n        x = self.patch_embed(x)\n\n        # Add cls token\n        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n        x = torch.cat((cls_token, x), dim=1)\n\n        # Add position embedding\n        x = x + self.pos_embed\n        x = self.pos_drop(x)\n\n        # Apply transformer blocks\n        for block in self.blocks:\n            x = block(x)\n\n        # Apply normalization\n        x = self.norm(x)\n\n        # Use [CLS] token for features\n        x = x[:, 0]\n        x = self.fc_features(x)\n        return x\n\n    def forward(self, x):\n        features = self.forward_features(x)\n        x = self.fc_classifier(features)\n        return x, features\n\n\n# --- MODEL FACTORY ---\ndef create_model(model_type, feature_dim=512, num_classes=10, use_attention=False):\n    \"\"\"Create a model based on the model_type parameter\"\"\"\n    if model_type == \"resnet18\":\n        return ResNet3D([2, 2, 2, 2], feature_dim=feature_dim, num_classes=num_classes, use_attention=use_attention)\n    elif model_type == \"resnet34\":\n        return ResNet3D([3, 4, 6, 3], feature_dim=feature_dim, num_classes=num_classes, use_attention=use_attention)\n    elif model_type == \"resnet50\":\n        return ResNet3D([3, 4, 6, 3], feature_dim=feature_dim, num_classes=num_classes, use_attention=use_attention)\n    elif model_type == \"vit\":\n        # Smaller ViT for Google Colab to avoid OOM issues\n        return VisionTransformer3D(vol_size=64, patch_size=8, in_chans=1, num_classes=num_classes,\n                                 embed_dim=384, depth=6, num_heads=6, feature_dim=feature_dim, drop_rate=0.1)\n    else:\n        raise ValueError(f\"Unsupported model type: {model_type}\")\n\n\n# --- PRETRAIN FUNCTION (SIMPLIFIED FOR COLAB) ---\ndef pretrain_model(model, train_loader, val_loader, num_epochs=5, device=\"cuda\"):\n    \"\"\"Pretrain the model on a classification task (simplified for Colab)\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(num_epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        correct = 0\n        total = 0\n\n        for voxels, labels, _ in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n            voxels, labels = voxels.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs, _ = model(voxels)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n\n        train_loss = train_loss / len(train_loader)\n        train_acc = 100. * correct / total\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        correct = 0\n        total = 0\n\n        with torch.no_grad():\n            for voxels, labels, _ in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n                voxels, labels = voxels.to(device), labels.to(device)\n\n                outputs, _ = model(voxels)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n\n        val_loss = val_loss / len(val_loader)\n        val_acc = 100. * correct / total\n\n        # Print status\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n\n    return model\n\n\n# --- FEATURE EXTRACTION FUNCTION ---\ndef extract_features(model, dataloader, output_folder):\n    \"\"\"Extract features from the model and save them to the output folder\"\"\"\n    model.eval()\n    start_time = time.time()\n\n    with torch.no_grad():\n        for voxels, filenames in tqdm(dataloader, desc=\"Extracting Features\"):\n            voxels = voxels.to(device)\n\n            # For classification dataset\n            if isinstance(filenames, tuple) and len(filenames) == 2:\n                _, filenames = filenames\n\n            # Extract features\n            if isinstance(model, VisionTransformer3D):\n                features = model.forward_features(voxels)\n            else:\n                features = model.forward_features(voxels)\n\n            # Save features for each sample\n            for i, filename in enumerate(filenames):\n                # Remove file extension and replace with .npy\n                base_name = os.path.splitext(os.path.basename(filename))[0]\n                feature_path = os.path.join(output_folder, f\"{base_name}.npy\")\n                np.save(feature_path, features[i].cpu().numpy())\n\n    extraction_time = time.time() - start_time\n    print(f\"✅ Features extracted in {extraction_time:.2f}s and saved to {output_folder}\")\n\n\n# --- FEATURE ANALYSIS FUNCTION ---\ndef analyze_features(output_folder, architecture_name, use_attention=False, n_samples=10):\n    \"\"\"Analyze the extracted features to check their quality\"\"\"\n    print(f\"\\nAnalyzing extracted features for {architecture_name} (attention={use_attention})...\")\n    feature_files = os.listdir(output_folder)\n\n    if len(feature_files) == 0:\n        print(\"No feature files found!\")\n        return\n\n    # Load a sample of feature files\n    sample_files = np.random.choice(feature_files, min(n_samples, len(feature_files)), replace=False)\n    features = []\n\n    for file in sample_files:\n        feature_path = os.path.join(output_folder, file)\n        feature = np.load(feature_path)\n        features.append(feature)\n\n    features = np.array(features)\n\n    # Print some statistics\n    print(f\"Feature shape: {features[0].shape}\")\n    print(f\"Feature mean: {np.mean(features):.4f}\")\n    print(f\"Feature std: {np.std(features):.4f}\")\n    print(f\"Feature min: {np.min(features):.4f}\")\n    print(f\"Feature max: {np.max(features):.4f}\")\n\n    # Plot feature distribution\n    plt.figure(figsize=(10, 6))\n    plt.hist(features.flatten(), bins=50)\n    plt.title(f\"Feature Distribution - {architecture_name} (Attention={use_attention})\")\n    plt.xlabel(\"Feature Value\")\n    plt.ylabel(\"Frequency\")\n    plt.savefig(f\"feature_distribution_{architecture_name}_attn_{use_attention}.png\")\n\n    # Plot feature heatmap for first few dimensions\n    if features[0].shape[0] > 10:\n        plt.figure(figsize=(12, 8))\n        plt.imshow(features[:, :10], aspect='auto', cmap='viridis')\n        plt.colorbar()\n        plt.title(f\"Feature Heatmap - {architecture_name} (Attention={use_attention})\")\n        plt.xlabel(\"Feature Dimension\")\n        plt.ylabel(\"Sample Index\")\n        plt.savefig(f\"feature_heatmap_{architecture_name}_attn_{use_attention}.png\")\n\n    plt.close('all')\n\n    # Return statistics for comparison\n    return {\n        \"architecture\": architecture_name,\n        \"attention\": use_attention,\n        \"mean\": np.mean(features),\n        \"std\": np.std(features),\n        \"min\": np.min(features),\n        \"max\": np.max(features),\n        \"feature_dim\": features[0].shape[0]\n    }\n\n\n# --- MAIN FUNCTION ---\ndef main():\n    results = []\n\n    # Create dataset only once\n    full_dataset = VoxelDataset(args.csv_file, args.voxel_folder, classification=args.pretrain)\n    print(f\"Total samples: {len(full_dataset)}\")\n\n    if args.pretrain:\n        # Split dataset into train and validation\n        train_size = int(0.8 * len(full_dataset))\n        val_size = len(full_dataset) - train_size\n        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=args.num_workers,\n            pin_memory=True if torch.cuda.is_available() else False\n        )\n\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.num_workers,\n            pin_memory=True if torch.cuda.is_available() else False\n        )\n\n    # Create DataLoader for feature extraction (once)\n    feature_dataset = VoxelDataset(args.csv_file, args.voxel_folder, classification=False)\n    feature_loader = DataLoader(\n        feature_dataset,\n        batch_size=args.batch_size,\n        shuffle=False,\n        num_workers=args.num_workers,\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    # Run each architecture\n    for architecture in ARCHITECTURES:\n        model_type = architecture[\"name\"]\n        use_attention = architecture[\"use_attention\"]\n\n        # Define folder for this architecture\n        arch_name = f\"{model_type}_attn_{use_attention}\"\n        output_folder = os.path.join(base_output_folder, arch_name)\n        os.makedirs(output_folder, exist_ok=True)\n\n        print(f\"\\n{'='*80}\")\n        print(f\"Running architecture: {model_type} with attention={use_attention}\")\n        print(f\"{'='*80}\")\n\n        # Create model\n        num_classes = 10  # Default number of classes for pretraining\n        model = create_model(model_type, args.feature_dim, num_classes, use_attention)\n        model.to(device)\n\n        # Pretrain if specified\n        if args.pretrain:\n            print(f\"Pretraining {model_type} with attention={use_attention}...\")\n            model = pretrain_model(model, train_loader, val_loader, args.pretrain_epochs, device)\n            # Save model checkpoint\n            save_path = f\"{model_type}_attn_{use_attention}_{args.save_model}\"\n            torch.save(model.state_dict(), save_path)\n            print(f\"Saved model checkpoint to {save_path}\")\n\n        # Extract features\n        extract_features(model, feature_loader, output_folder)\n\n        # Analyze features and collect results\n        stats = analyze_features(output_folder, model_type, use_attention)\n        results.append(stats)\n\n        # Clear memory\n        del model\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n    # Compare results\n    print(\"\\n========== COMPARISON OF ALL ARCHITECTURES ==========\")\n    print(\"{:<12} {:<8} {:<8} {:<8} {:<8} {:<8}\".format(\n        \"Architecture\", \"Attention\", \"Mean\", \"Std\", \"Min\", \"Max\"))\n    print(\"-\" * 60)\n\n    for result in results:\n        print(\"{:<12} {:<8} {:<8.4f} {:<8.4f} {:<8.4f} {:<8.4f}\".format(\n            result[\"architecture\"],\n            str(result[\"attention\"]),\n            result[\"mean\"],\n            result[\"std\"],\n            result[\"min\"],\n            result[\"max\"]))\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary libraries\n! pip install torch torchvision torchaudio nltk\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import TransformerDecoder, TransformerDecoderLayer\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.optim as optim\nfrom nltk.translate.bleu_score import sentence_bleu, corpus_bleu\nimport matplotlib.pyplot as plt\nimport nltk\n\n# Download necessary NLTK data\ntry:\n    nltk.download('punkt')\nexcept:\n    pass\n\n# Set random seeds for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\n# Paths\ncsv_file = \"/kaggle/input/3d-p-structures/pdb_voxel_mapping.csv\"\nfeature_folder = \"/kaggle/input/3d-p-structures/extracted_features/content/extracted_features\"\n\n# Check CSV format properly\ndef check_csv(csv_file):\n    print(f\"\\nChecking CSV file: {csv_file}\")\n    if os.path.exists(csv_file):\n        # First try reading with header\n        df = pd.read_csv(csv_file)\n        print(f\"CSV shape with header: {df.shape}\")\n        print(\"First 5 rows (with header):\")\n        print(df.head())\n\n        # Also try reading without header for comparison\n        df_no_header = pd.read_csv(csv_file, header=None)\n        print(f\"\\nCSV shape without header: {df_no_header.shape}\")\n        print(\"First 5 rows (without header):\")\n        print(df_no_header.head())\n\n        return df\n    else:\n        print(\"CSV file does not exist!\")\n        return None\n\n# Check the structure of the feature folder\ndef inspect_feature_folder(feature_folder):\n    print(f\"\\nInspecting feature folder: {feature_folder}\")\n\n    if not os.path.exists(feature_folder):\n        print(\"Feature folder does not exist!\")\n        return []\n\n    all_files = os.listdir(feature_folder)\n    print(f\"Total files in folder: {len(all_files)}\")\n\n    # Show file extensions\n    extensions = {}\n    for file in all_files:\n        ext = os.path.splitext(file)[1]\n        extensions[ext] = extensions.get(ext, 0) + 1\n\n    print(\"File extensions:\")\n    for ext, count in extensions.items():\n        print(f\"  {ext}: {count} files\")\n\n    # Show some example files\n    print(\"\\nSample files:\")\n    for file in all_files[:5]:\n        full_path = os.path.join(feature_folder, file)\n        size = os.path.getsize(full_path)\n        print(f\"  - {file} ({size} bytes)\")\n\n    return all_files\n\n# Simple tokenizer\nclass SimpleTokenizer:\n    def __init__(self, min_freq=1, max_vocab=10000):\n        self.word2idx = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3}\n        self.idx2word = {0: '<pad>', 1: '<start>', 2: '<end>', 3: '<unk>'}\n        self.min_freq = min_freq\n        self.max_vocab = max_vocab\n        self.counter = Counter()\n        self.built = False\n\n    def preprocess(self, text):\n        # Handle non-string inputs\n        if not isinstance(text, str):\n            text = str(text)\n        # Simple preprocessing: lowercase and split by space\n        return text.lower().split()\n\n    def update_counter(self, texts):\n        for text in texts:\n            tokens = self.preprocess(text)\n            self.counter.update(tokens)\n\n    def build_vocab(self):\n        # Keep only frequent words\n        words = [word for word, count in self.counter.items()\n                 if count >= self.min_freq]\n\n        # Limit vocabulary size\n        if len(words) > self.max_vocab:\n            words = sorted([(word, count) for word, count in self.counter.items()\n                            if count >= self.min_freq],\n                           key=lambda x: x[1], reverse=True)[:self.max_vocab]\n            words = [word for word, _ in words]\n\n        # Add words to vocabulary\n        for word in words:\n            idx = len(self.word2idx)\n            self.word2idx[word] = idx\n            self.idx2word[idx] = word\n\n        self.built = True\n\n    def encode(self, text, max_length=None):\n        if not self.built:\n            raise ValueError(\"Vocabulary not built yet. Call build_vocab() first.\")\n\n        tokens = self.preprocess(text)\n        token_ids = [self.word2idx.get(token, self.word2idx['<unk>']) for token in tokens]\n\n        # Add start and end tokens\n        token_ids = [self.word2idx['<start>']] + token_ids + [self.word2idx['<end>']]\n\n        # Apply max_length if specified\n        if max_length is not None:\n            if len(token_ids) > max_length:\n                token_ids = token_ids[:max_length]\n            else:\n                token_ids = token_ids + [self.word2idx['<pad>']] * (max_length - len(token_ids))\n\n        return token_ids\n\n    def decode(self, token_ids):\n        return [self.idx2word.get(idx, '<unk>') for idx in token_ids]\n\n    def __len__(self):\n        return len(self.word2idx)\n\nclass ProteinStructureCaptioningDataset(Dataset):\n    def __init__(self, csv_file, feature_folder, tokenizer, max_caption_length=50):\n        self.df = pd.read_csv(csv_file)\n        self.feature_folder = feature_folder\n        self.tokenizer = tokenizer\n        self.max_caption_length = max_caption_length\n        self.voxel_to_feature = self._map_voxel_to_feature()\n        self.caption_column = self._get_caption_column()\n        self.voxel_column = self._get_voxel_column()\n\n    def _get_caption_column(self):\n        if 'description' in self.df.columns:\n            return 'description'\n        elif 1 in self.df.columns:\n            return 1\n        else:\n            return self.df.columns[1]\n\n    def _get_voxel_column(self):\n        if 'voxel_file' in self.df.columns:\n            return 'voxel_file'\n        elif 0 in self.df.columns:\n            return 0\n        else:\n            return self.df.columns[0]\n\n    def _map_voxel_to_feature(self):\n        mapping = {}\n        for filename in os.listdir(self.feature_folder):\n            base_name = os.path.splitext(filename)[0]\n            mapping[base_name] = os.path.join(self.feature_folder, filename)\n        return mapping\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        voxel_file = row[self.voxel_column]\n        caption = row[self.caption_column]\n\n        base_name = os.path.splitext(os.path.basename(voxel_file))[0]\n        feature_path = self.voxel_to_feature.get(base_name)\n        if feature_path and os.path.exists(feature_path):\n            features = np.load(feature_path)\n            # Assuming features are 2D, flatten them for simplicity as input to Transformer\n            features = features.flatten()\n        else:\n            # Handle missing feature files appropriately\n            features = np.zeros((2048,)) # Example: initialize with zeros\n\n        token_ids = self.tokenizer.encode(caption, max_length=self.max_caption_length)\n        return torch.tensor(features, dtype=torch.float32), torch.tensor(token_ids, dtype=torch.long)\n\ndef collate_fn(batch):\n    features, captions = zip(*batch)\n    features = torch.stack(features, 0)\n    # Pad captions\n    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n    return features, captions\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n\nclass TransformerCaptioner(nn.Module):\n    def __init__(self, feature_size, embed_size, hidden_size, vocab_size, num_heads=8, num_layers=6, dropout=0.1, max_len=50):\n        super(TransformerCaptioner, self).__init__()\n        self.embed = nn.Embedding(vocab_size, embed_size)\n        self.pos_encoder = PositionalEncoding(embed_size, dropout, max_len)\n        decoder_layer = TransformerDecoderLayer(embed_size, num_heads, hidden_size, dropout)\n        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers)\n        self.linear = nn.Linear(embed_size, vocab_size)\n        self.feature_proj = nn.Linear(feature_size, embed_size) # Project features to embedding size\n        self.dropout = nn.Dropout(dropout)\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n\n    def forward(self, features, captions):\n        # Project features and add positional encoding (treating features as a sequence of length 1)\n        memory = self.dropout(self.feature_proj(features).unsqueeze(1)) # (batch_size, 1, embed_size)\n        \n        # Transpose memory to match transformer decoder's expected shape [seq_len, batch_size, embed_dim]\n        memory = memory.transpose(0, 1)\n\n        # Embed and add positional encoding to captions\n        tgt_embeddings = self.dropout(self.embed(captions)) * torch.sqrt(torch.tensor(self.embed.embedding_dim, dtype=torch.float))\n        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n        \n        # Transpose target embeddings to match transformer decoder's expected shape [seq_len, batch_size, embed_dim]\n        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n\n        # Generate the mask for the target sequence based on the current caption length\n        tgt_mask = self._generate_square_subsequent_mask(tgt_embeddings.size(0)).to(captions.device)\n\n        # Decode\n        output = self.transformer_decoder(tgt_embeddings, memory, tgt_mask=tgt_mask)\n        \n        # Transpose output back to [batch_size, seq_len, embed_size]\n        output = output.transpose(0, 1)\n        \n        # Apply linear projection to get logits\n        output = self.linear(output)\n        return output\n\n    def generate_caption(self, features, start_token_idx, end_token_idx, max_len=50):\n        \"\"\"Generate caption using greedy decoding.\"\"\"\n        # Project features\n        memory = self.dropout(self.feature_proj(features).unsqueeze(1))\n        # Transpose to [seq_len, batch_size, embed_dim]\n        memory = memory.transpose(0, 1)\n        \n        batch_size = features.size(0)\n        tgt = torch.full((batch_size, 1), start_token_idx, dtype=torch.long).to(features.device)\n\n        for i in range(max_len - 1):\n            # Embed target tokens\n            tgt_embeddings = self.dropout(self.embed(tgt)) * torch.sqrt(torch.tensor(self.embed.embedding_dim, dtype=torch.float))\n            tgt_embeddings = self.pos_encoder(tgt_embeddings)\n            \n            # Transpose to [seq_len, batch_size, embed_dim]\n            tgt_embeddings = tgt_embeddings.transpose(0, 1)\n            \n            # Generate appropriate mask\n            tgt_mask = self._generate_square_subsequent_mask(tgt_embeddings.size(0)).to(tgt.device)\n            \n            # Pass through transformer decoder\n            output = self.transformer_decoder(tgt_embeddings, memory, tgt_mask=tgt_mask)\n            \n            # Transpose output back to [batch_size, seq_len, embed_dim]\n            output = output.transpose(0, 1)\n            \n            # Get next word prediction\n            prob = F.softmax(self.linear(output[:, -1, :]), dim=-1)\n            next_word = torch.argmax(prob, dim=-1).unsqueeze(1)\n            tgt = torch.cat((tgt, next_word), dim=1)\n            \n            # Stop if all sequences have end token\n            if (next_word == end_token_idx).all():\n                break\n                \n        return tgt\n\n    def _generate_square_subsequent_mask(self, sz):\n        \"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n        Unmasked positions are filled with float(0.0).\n        \"\"\"\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\ndef evaluate_bleu(model, dataloader, tokenizer, device, max_len=50):\n    \"\"\"\n    Evaluate the model using BLEU score on the validation set.\n    \n    Args:\n        model: The trained captioning model\n        dataloader: DataLoader containing validation data\n        tokenizer: Tokenizer used for encoding/decoding\n        device: Device to run the model on\n        max_len: Maximum length of generated captions\n        \n    Returns:\n        list: BLEU scores for different n-gram weights\n    \"\"\"\n    model.eval()\n    all_references = []\n    all_candidates = []\n    \n    start_token_idx = tokenizer.word2idx['<start>']\n    end_token_idx = tokenizer.word2idx['<end>']\n    pad_token_idx = tokenizer.word2idx['<pad>']\n    \n    with torch.no_grad():\n        for features, captions in dataloader:\n            features = features.to(device)\n            \n            # Generate captions\n            generated_captions = model.generate_caption(\n                features, start_token_idx, end_token_idx, max_len=max_len\n            )\n            \n            # Process each caption in the batch\n            for i, (generated, reference) in enumerate(zip(generated_captions, captions)):\n                # Convert tensors to lists and remove special tokens\n                candidate = [\n                    tokenizer.idx2word[idx.item()] for idx in generated \n                    if idx.item() not in [start_token_idx, end_token_idx, pad_token_idx]\n                ]\n                \n                # Get reference without special tokens\n                reference_tokens = [\n                    tokenizer.idx2word[idx.item()] for idx in reference \n                    if idx.item() not in [start_token_idx, end_token_idx, pad_token_idx]\n                ]\n                \n                # Add to lists for corpus BLEU calculation\n                all_references.append([reference_tokens])\n                all_candidates.append(candidate)\n    \n    # Calculate BLEU-1, BLEU-2, BLEU-3, and BLEU-4 scores\n    weights = [(1, 0, 0, 0), (0.5, 0.5, 0, 0), (0.33, 0.33, 0.33, 0), (0.25, 0.25, 0.25, 0.25)]\n    bleu_scores = []\n    \n    for weight in weights:\n        score = corpus_bleu(all_references, all_candidates, weights=weight)\n        bleu_scores.append(score)\n    \n    return bleu_scores\n\ndef analyze_bleu_scores(model, dataloader, tokenizer, device, max_len=50, num_examples=5):\n    \"\"\"\n    Analyze BLEU scores distribution and show examples of high and low scoring captions.\n    \n    Args:\n        model: The trained captioning model\n        dataloader: DataLoader containing validation data\n        tokenizer: Tokenizer used for encoding/decoding\n        device: Device to run the model on\n        max_len: Maximum length of generated captions\n        num_examples: Number of examples to show for high and low scores\n    \"\"\"\n    model.eval()\n    item_scores = []\n    all_references = []\n    all_candidates = []\n    all_features = []\n    \n    start_token_idx = tokenizer.word2idx['<start>']\n    end_token_idx = tokenizer.word2idx['<end>']\n    pad_token_idx = tokenizer.word2idx['<pad>']\n    \n    with torch.no_grad():\n        for features, captions in dataloader:\n            features = features.to(device)\n            \n            # Generate captions\n            generated_captions = model.generate_caption(\n                features, start_token_idx, end_token_idx, max_len=max_len\n            )\n            \n            # Process each caption in the batch\n            for i, (feature, generated, reference) in enumerate(zip(features, generated_captions, captions)):\n                # Convert tensors to lists and remove special tokens\n                candidate = [\n                    tokenizer.idx2word[idx.item()] for idx in generated \n                    if idx.item() not in [start_token_idx, end_token_idx, pad_token_idx]\n                ]\n                \n                # Get reference without special tokens\n                reference_tokens = [\n                    tokenizer.idx2word[idx.item()] for idx in reference \n                    if idx.item() not in [start_token_idx, end_token_idx, pad_token_idx]\n                ]\n                \n                # Calculate BLEU-4 score for this example\n                bleu4_score = sentence_bleu([reference_tokens], candidate, \n                                           weights=(0.25, 0.25, 0.25, 0.25))\n                \n                item_scores.append(bleu4_score)\n                all_references.append(reference_tokens)\n                all_candidates.append(candidate)\n                all_features.append(feature.cpu())\n    \n    # Plot histogram of BLEU scores\n    plt.figure(figsize=(10, 6))\n    plt.hist(item_scores, bins=20)\n    plt.title('Distribution of BLEU-4 Scores')\n    plt.xlabel('BLEU-4 Score')\n    plt.ylabel('Frequency')\n    plt.savefig('bleu_score_distribution.png')\n    \n    # Find examples with highest and lowest scores\n    scores_with_idx = [(score, idx) for idx, score in enumerate(item_scores)]\n    highest_scores = sorted(scores_with_idx, reverse=True)[:num_examples]\n    lowest_scores = sorted(scores_with_idx)[:num_examples]\n    \n    print(\"\\n=== Examples with Highest BLEU-4 Scores ===\")\n    for score, idx in highest_scores:\n        print(f\"BLEU-4 Score: {score:.4f}\")\n        print(f\"Reference: {' '.join(all_references[idx])}\")\n        print(f\"Generated: {' '.join(all_candidates[idx])}\")\n        print(\"---\")\n    \n    print(\"\\n=== Examples with Lowest BLEU-4 Scores ===\")\n    for score, idx in lowest_scores:\n        print(f\"BLEU-4 Score: {score:.4f}\")\n        print(f\"Reference: {' '.join(all_references[idx])}\")\n        print(f\"Generated: {' '.join(all_candidates[idx])}\")\n        print(\"---\")\n    \n    # Calculate average BLEU score\n    avg_bleu4 = sum(item_scores) / len(item_scores)\n    print(f\"\\nAverage BLEU-4 Score: {avg_bleu4:.4f}\")\n\n# Main execution block\ntry:\n    # Check CSV file\n    df = check_csv(csv_file)\n\n    # Inspect feature folder\n    feature_files = inspect_feature_folder(feature_folder)\n\n    if df is not None and len(feature_files) > 0:\n        # Determine caption column\n        if 'description' in df.columns:\n            caption_column = 'description'\n        elif 1 in df.columns:\n            caption_column = 1\n        else:\n            caption_column = df.columns[1]\n\n        # Build vocabulary\n        tokenizer = SimpleTokenizer()\n        tokenizer.update_counter(df[caption_column].astype(str).tolist())\n        tokenizer.build_vocab()\n        print(f\"\\nVocabulary size: {len(tokenizer)}\")\n\n        # Create Dataset and split into train/validation\n        max_caption_length = 50\n        dataset = ProteinStructureCaptioningDataset(csv_file, feature_folder, tokenizer, max_caption_length)\n        \n        # Split dataset into train and validation sets\n        dataset_size = len(dataset)\n        train_size = int(dataset_size * 0.8)\n        val_size = dataset_size - train_size\n        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\n        train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n        val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n\n        # Model parameters\n        feature_size = dataset[0][0].shape[0] # Get feature size from the first item\n        embed_size = 256\n        hidden_size = 512\n        vocab_size = len(tokenizer)\n        num_heads = 8\n        num_layers = 3\n        dropout = 0.1\n        max_len = max_caption_length\n\n        # Instantiate the Transformer model\n        model = TransformerCaptioner(feature_size, embed_size, hidden_size, vocab_size, num_heads, num_layers, dropout, max_len)\n\n        # Define loss function and optimizer\n        criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.word2idx['<pad>'])\n        optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n        # Training loop\n        num_epochs = 50\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        model.to(device)\n\n        for epoch in range(num_epochs):\n            # Training phase\n            model.train()\n            total_loss = 0\n            for batch_idx, (features, captions) in enumerate(train_dataloader):\n                features = features.to(device)\n                captions = captions.to(device)\n\n                optimizer.zero_grad()\n                outputs = model(features, captions[:, :-1])  # Exclude <end> token for input\n                targets = captions[:, 1:]  # Exclude <start> token for target\n                \n                # Reshape outputs and targets for loss calculation\n                outputs = outputs.reshape(-1, vocab_size)\n                targets = targets.reshape(-1)\n\n                loss = criterion(outputs, targets)\n                loss.backward()\n                optimizer.step()\n\n                total_loss += loss.item()\n\n                if (batch_idx + 1) % 10 == 0:\n                    print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n\n            avg_loss = total_loss / len(train_dataloader)\n            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n            \n            # Evaluate with BLEU score\n            bleu_scores = evaluate_bleu(model, val_dataloader, tokenizer, device, max_len=max_caption_length)\n            print(f'BLEU-1: {bleu_scores[0]:.4f}, BLEU-2: {bleu_scores[1]:.4f}, BLEU-3: {bleu_scores[2]:.4f}, BLEU-4: {bleu_scores[3]:.4f}')\n\n        # After training, analyze BLEU scores in detail\n        analyze_bleu_scores(model, val_dataloader, tokenizer, device, max_len=max_caption_length)\n\n        # Example of generating a caption for the first item in the dataset\n        model.eval()\n        with torch.no_grad():\n            first_feature = dataset[0][0].unsqueeze(0).to(device)\n            start_token_idx = tokenizer.word2idx['<start>']\n            end_token_idx = tokenizer.word2idx['<end>']\n            generated_tokens = model.generate_caption(first_feature, start_token_idx, end_token_idx, max_len=max_caption_length)[0].cpu().numpy()\n            generated_caption = tokenizer.decode([token for token in generated_tokens if token not in [tokenizer.word2idx['<start>'], tokenizer.word2idx['<end>'], tokenizer.word2idx['<pad>']]])\n            print(\"\\nGenerated Caption (Example):\", \" \".join(generated_caption))\n\nexcept Exception as e:\n    print(f\"Error occurred: {str(e)}\")\n    import traceback\n    traceback.print_exc()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-29T12:06:05.456252Z","iopub.execute_input":"2025-04-29T12:06:05.456926Z","iopub.status.idle":"2025-04-29T12:11:50.000002Z","shell.execute_reply.started":"2025-04-29T12:06:05.456900Z","shell.execute_reply":"2025-04-29T12:11:49.999289Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n\nChecking CSV file: /kaggle/input/3d-p-structures/pdb_voxel_mapping.csv\nCSV shape with header: (3000, 2)\nFirst 5 rows (with header):\n          Voxel File                                    Structure Title\n0  pdb1c3y_voxel.pkl        THP12-CARRIER PROTEIN FROM YELLOW MEAL WORM\n1  pdb1g3o_voxel.pkl   CRYSTAL STRUCTURE OF V19E MUTANT OF FERREDOXIN I\n2  pdb1fu7_voxel.pkl  STRUCTURES OF GLYCOGEN PHOSPHORYLASE-INHIBITOR...\n3  pdb1iav_voxel.pkl  STRUCTURE ON NATIVE (ASN 87) SUBTILISIN FROM B...\n4  pdb1aks_voxel.pkl  CRYSTAL STRUCTURE OF THE FIRST ACTIVE AUTOLYSA...\n\nCSV shape without header: (3001, 2)\nFirst 5 rows (without header):\n                   0                                                  1\n0         Voxel File                                    Structure Title\n1  pdb1c3y_voxel.pkl        THP12-CARRIER PROTEIN FROM YELLOW MEAL WORM\n2  pdb1g3o_voxel.pkl   CRYSTAL STRUCTURE OF V19E MUTANT OF FERREDOXIN I\n3  pdb1fu7_voxel.pkl  STRUCTURES OF GLYCOGEN PHOSPHORYLASE-INHIBITOR...\n4  pdb1iav_voxel.pkl  STRUCTURE ON NATIVE (ASN 87) SUBTILISIN FROM B...\n\nInspecting feature folder: /kaggle/input/3d-p-structures/extracted_features/content/extracted_features\nTotal files in folder: 3000\nFile extensions:\n  .npy: 3000 files\n\nSample files:\n  - pdb1i3l_voxel.npy (2176 bytes)\n  - pdb1his_voxel.npy (2176 bytes)\n  - pdb1g02_voxel.npy (2176 bytes)\n  - pdb1cc7_voxel.npy (2176 bytes)\n  - pdb1gz0_voxel.npy (2176 bytes)\n\nVocabulary size: 5072\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/50], Step [10/75], Loss: 6.1707\nEpoch [1/50], Step [20/75], Loss: 5.8385\nEpoch [1/50], Step [30/75], Loss: 5.4099\nEpoch [1/50], Step [40/75], Loss: 5.6213\nEpoch [1/50], Step [50/75], Loss: 5.2885\nEpoch [1/50], Step [60/75], Loss: 5.2431\nEpoch [1/50], Step [70/75], Loss: 5.8134\nEpoch [1/50], Average Loss: 5.7439\nBLEU-1: 0.1833, BLEU-2: 0.1228, BLEU-3: 0.0835, BLEU-4: 0.0435\nEpoch [2/50], Step [10/75], Loss: 4.8279\nEpoch [2/50], Step [20/75], Loss: 4.5026\nEpoch [2/50], Step [30/75], Loss: 4.6697\nEpoch [2/50], Step [40/75], Loss: 4.7957\nEpoch [2/50], Step [50/75], Loss: 5.2209\nEpoch [2/50], Step [60/75], Loss: 4.9982\nEpoch [2/50], Step [70/75], Loss: 4.3747\nEpoch [2/50], Average Loss: 4.7462\nBLEU-1: 0.2192, BLEU-2: 0.1616, BLEU-3: 0.1184, BLEU-4: 0.0676\nEpoch [3/50], Step [10/75], Loss: 4.4067\nEpoch [3/50], Step [20/75], Loss: 4.0478\nEpoch [3/50], Step [30/75], Loss: 4.1963\nEpoch [3/50], Step [40/75], Loss: 4.4442\nEpoch [3/50], Step [50/75], Loss: 4.2046\nEpoch [3/50], Step [60/75], Loss: 4.0474\nEpoch [3/50], Step [70/75], Loss: 4.2920\nEpoch [3/50], Average Loss: 4.1727\nBLEU-1: 0.2198, BLEU-2: 0.1547, BLEU-3: 0.1093, BLEU-4: 0.0598\nEpoch [4/50], Step [10/75], Loss: 3.7779\nEpoch [4/50], Step [20/75], Loss: 3.6649\nEpoch [4/50], Step [30/75], Loss: 3.8544\nEpoch [4/50], Step [40/75], Loss: 3.9413\nEpoch [4/50], Step [50/75], Loss: 3.6654\nEpoch [4/50], Step [60/75], Loss: 4.0317\nEpoch [4/50], Step [70/75], Loss: 3.7487\nEpoch [4/50], Average Loss: 3.6650\nBLEU-1: 0.1740, BLEU-2: 0.1201, BLEU-3: 0.0827, BLEU-4: 0.0434\nEpoch [5/50], Step [10/75], Loss: 2.9597\nEpoch [5/50], Step [20/75], Loss: 3.1103\nEpoch [5/50], Step [30/75], Loss: 3.2934\nEpoch [5/50], Step [40/75], Loss: 3.3155\nEpoch [5/50], Step [50/75], Loss: 3.2413\nEpoch [5/50], Step [60/75], Loss: 3.1781\nEpoch [5/50], Step [70/75], Loss: 3.3553\nEpoch [5/50], Average Loss: 3.2206\nBLEU-1: 0.2205, BLEU-2: 0.1582, BLEU-3: 0.1129, BLEU-4: 0.0624\nEpoch [6/50], Step [10/75], Loss: 2.7054\nEpoch [6/50], Step [20/75], Loss: 2.7523\nEpoch [6/50], Step [30/75], Loss: 2.8763\nEpoch [6/50], Step [40/75], Loss: 2.7026\nEpoch [6/50], Step [50/75], Loss: 3.1020\nEpoch [6/50], Step [60/75], Loss: 2.9211\nEpoch [6/50], Step [70/75], Loss: 2.7372\nEpoch [6/50], Average Loss: 2.8201\nBLEU-1: 0.1265, BLEU-2: 0.0852, BLEU-3: 0.0577, BLEU-4: 0.0296\nEpoch [7/50], Step [10/75], Loss: 2.1616\nEpoch [7/50], Step [20/75], Loss: 2.4840\nEpoch [7/50], Step [30/75], Loss: 2.3646\nEpoch [7/50], Step [40/75], Loss: 2.4762\nEpoch [7/50], Step [50/75], Loss: 2.4494\nEpoch [7/50], Step [60/75], Loss: 2.6136\nEpoch [7/50], Step [70/75], Loss: 2.3943\nEpoch [7/50], Average Loss: 2.4626\nBLEU-1: 0.2002, BLEU-2: 0.1334, BLEU-3: 0.0912, BLEU-4: 0.0497\nEpoch [8/50], Step [10/75], Loss: 1.8606\nEpoch [8/50], Step [20/75], Loss: 2.2172\nEpoch [8/50], Step [30/75], Loss: 2.1062\nEpoch [8/50], Step [40/75], Loss: 2.2080\nEpoch [8/50], Step [50/75], Loss: 2.2193\nEpoch [8/50], Step [60/75], Loss: 2.2854\nEpoch [8/50], Step [70/75], Loss: 2.2067\nEpoch [8/50], Average Loss: 2.1639\nBLEU-1: 0.1879, BLEU-2: 0.1320, BLEU-3: 0.0919, BLEU-4: 0.0499\nEpoch [9/50], Step [10/75], Loss: 1.8594\nEpoch [9/50], Step [20/75], Loss: 1.9968\nEpoch [9/50], Step [30/75], Loss: 1.9469\nEpoch [9/50], Step [40/75], Loss: 1.9850\nEpoch [9/50], Step [50/75], Loss: 2.0158\nEpoch [9/50], Step [60/75], Loss: 1.8419\nEpoch [9/50], Step [70/75], Loss: 2.0373\nEpoch [9/50], Average Loss: 1.9449\nBLEU-1: 0.1910, BLEU-2: 0.1332, BLEU-3: 0.0940, BLEU-4: 0.0527\nEpoch [10/50], Step [10/75], Loss: 1.5852\nEpoch [10/50], Step [20/75], Loss: 1.6031\nEpoch [10/50], Step [30/75], Loss: 1.6882\nEpoch [10/50], Step [40/75], Loss: 1.7769\nEpoch [10/50], Step [50/75], Loss: 1.6635\nEpoch [10/50], Step [60/75], Loss: 2.0313\nEpoch [10/50], Step [70/75], Loss: 1.8424\nEpoch [10/50], Average Loss: 1.7582\nBLEU-1: 0.2023, BLEU-2: 0.1424, BLEU-3: 0.1009, BLEU-4: 0.0571\nEpoch [11/50], Step [10/75], Loss: 1.6682\nEpoch [11/50], Step [20/75], Loss: 1.4803\nEpoch [11/50], Step [30/75], Loss: 1.5438\nEpoch [11/50], Step [40/75], Loss: 1.6348\nEpoch [11/50], Step [50/75], Loss: 1.7233\nEpoch [11/50], Step [60/75], Loss: 1.6870\nEpoch [11/50], Step [70/75], Loss: 1.7507\nEpoch [11/50], Average Loss: 1.6049\nBLEU-1: 0.2129, BLEU-2: 0.1431, BLEU-3: 0.1005, BLEU-4: 0.0570\nEpoch [12/50], Step [10/75], Loss: 1.4606\nEpoch [12/50], Step [20/75], Loss: 1.3948\nEpoch [12/50], Step [30/75], Loss: 1.3349\nEpoch [12/50], Step [40/75], Loss: 1.4566\nEpoch [12/50], Step [50/75], Loss: 1.5646\nEpoch [12/50], Step [60/75], Loss: 1.5743\nEpoch [12/50], Step [70/75], Loss: 1.6230\nEpoch [12/50], Average Loss: 1.4811\nBLEU-1: 0.2069, BLEU-2: 0.1485, BLEU-3: 0.1065, BLEU-4: 0.0613\nEpoch [13/50], Step [10/75], Loss: 1.2625\nEpoch [13/50], Step [20/75], Loss: 1.2630\nEpoch [13/50], Step [30/75], Loss: 1.4051\nEpoch [13/50], Step [40/75], Loss: 1.3506\nEpoch [13/50], Step [50/75], Loss: 1.4800\nEpoch [13/50], Step [60/75], Loss: 1.5577\nEpoch [13/50], Step [70/75], Loss: 1.5036\nEpoch [13/50], Average Loss: 1.4022\nBLEU-1: 0.1926, BLEU-2: 0.1384, BLEU-3: 0.0996, BLEU-4: 0.0568\nEpoch [14/50], Step [10/75], Loss: 1.2385\nEpoch [14/50], Step [20/75], Loss: 1.2296\nEpoch [14/50], Step [30/75], Loss: 1.2801\nEpoch [14/50], Step [40/75], Loss: 1.2748\nEpoch [14/50], Step [50/75], Loss: 1.3369\nEpoch [14/50], Step [60/75], Loss: 1.4106\nEpoch [14/50], Step [70/75], Loss: 1.3311\nEpoch [14/50], Average Loss: 1.3100\nBLEU-1: 0.2118, BLEU-2: 0.1461, BLEU-3: 0.1031, BLEU-4: 0.0570\nEpoch [15/50], Step [10/75], Loss: 1.3061\nEpoch [15/50], Step [20/75], Loss: 1.2410\nEpoch [15/50], Step [30/75], Loss: 1.1843\nEpoch [15/50], Step [40/75], Loss: 1.2944\nEpoch [15/50], Step [50/75], Loss: 1.3070\nEpoch [15/50], Step [60/75], Loss: 1.3309\nEpoch [15/50], Step [70/75], Loss: 1.3650\nEpoch [15/50], Average Loss: 1.2600\nBLEU-1: 0.2187, BLEU-2: 0.1562, BLEU-3: 0.1127, BLEU-4: 0.0648\nEpoch [16/50], Step [10/75], Loss: 1.1146\nEpoch [16/50], Step [20/75], Loss: 1.0758\nEpoch [16/50], Step [30/75], Loss: 1.1116\nEpoch [16/50], Step [40/75], Loss: 1.2680\nEpoch [16/50], Step [50/75], Loss: 1.2882\nEpoch [16/50], Step [60/75], Loss: 1.2917\nEpoch [16/50], Step [70/75], Loss: 1.3018\nEpoch [16/50], Average Loss: 1.2121\nBLEU-1: 0.1963, BLEU-2: 0.1355, BLEU-3: 0.0940, BLEU-4: 0.0499\nEpoch [17/50], Step [10/75], Loss: 1.1761\nEpoch [17/50], Step [20/75], Loss: 1.1457\nEpoch [17/50], Step [30/75], Loss: 1.1805\nEpoch [17/50], Step [40/75], Loss: 1.2413\nEpoch [17/50], Step [50/75], Loss: 1.2185\nEpoch [17/50], Step [60/75], Loss: 1.3009\nEpoch [17/50], Step [70/75], Loss: 1.1924\nEpoch [17/50], Average Loss: 1.1806\nBLEU-1: 0.1892, BLEU-2: 0.1351, BLEU-3: 0.0970, BLEU-4: 0.0554\nEpoch [18/50], Step [10/75], Loss: 1.1221\nEpoch [18/50], Step [20/75], Loss: 1.0382\nEpoch [18/50], Step [30/75], Loss: 0.9826\nEpoch [18/50], Step [40/75], Loss: 1.1822\nEpoch [18/50], Step [50/75], Loss: 1.1449\nEpoch [18/50], Step [60/75], Loss: 1.2029\nEpoch [18/50], Step [70/75], Loss: 1.2531\nEpoch [18/50], Average Loss: 1.1468\nBLEU-1: 0.2006, BLEU-2: 0.1425, BLEU-3: 0.1025, BLEU-4: 0.0590\nEpoch [19/50], Step [10/75], Loss: 1.0477\nEpoch [19/50], Step [20/75], Loss: 1.0269\nEpoch [19/50], Step [30/75], Loss: 1.1035\nEpoch [19/50], Step [40/75], Loss: 1.1260\nEpoch [19/50], Step [50/75], Loss: 1.1547\nEpoch [19/50], Step [60/75], Loss: 1.1752\nEpoch [19/50], Step [70/75], Loss: 1.1418\nEpoch [19/50], Average Loss: 1.1187\nBLEU-1: 0.1978, BLEU-2: 0.1366, BLEU-3: 0.0963, BLEU-4: 0.0548\nEpoch [20/50], Step [10/75], Loss: 1.0504\nEpoch [20/50], Step [20/75], Loss: 0.9482\nEpoch [20/50], Step [30/75], Loss: 1.1217\nEpoch [20/50], Step [40/75], Loss: 1.1273\nEpoch [20/50], Step [50/75], Loss: 1.0574\nEpoch [20/50], Step [60/75], Loss: 1.2651\nEpoch [20/50], Step [70/75], Loss: 1.1551\nEpoch [20/50], Average Loss: 1.0886\nBLEU-1: 0.2088, BLEU-2: 0.1491, BLEU-3: 0.1071, BLEU-4: 0.0630\nEpoch [21/50], Step [10/75], Loss: 1.0037\nEpoch [21/50], Step [20/75], Loss: 1.0134\nEpoch [21/50], Step [30/75], Loss: 1.1216\nEpoch [21/50], Step [40/75], Loss: 0.9884\nEpoch [21/50], Step [50/75], Loss: 1.0941\nEpoch [21/50], Step [60/75], Loss: 1.1572\nEpoch [21/50], Step [70/75], Loss: 1.1311\nEpoch [21/50], Average Loss: 1.0818\nBLEU-1: 0.1991, BLEU-2: 0.1418, BLEU-3: 0.1020, BLEU-4: 0.0582\nEpoch [22/50], Step [10/75], Loss: 0.9089\nEpoch [22/50], Step [20/75], Loss: 1.0322\nEpoch [22/50], Step [30/75], Loss: 0.9912\nEpoch [22/50], Step [40/75], Loss: 1.0385\nEpoch [22/50], Step [50/75], Loss: 1.0250\nEpoch [22/50], Step [60/75], Loss: 1.0476\nEpoch [22/50], Step [70/75], Loss: 1.2267\nEpoch [22/50], Average Loss: 1.0520\nBLEU-1: 0.2055, BLEU-2: 0.1436, BLEU-3: 0.1020, BLEU-4: 0.0590\nEpoch [23/50], Step [10/75], Loss: 1.0573\nEpoch [23/50], Step [20/75], Loss: 0.9944\nEpoch [23/50], Step [30/75], Loss: 1.0541\nEpoch [23/50], Step [40/75], Loss: 0.9642\nEpoch [23/50], Step [50/75], Loss: 1.0029\nEpoch [23/50], Step [60/75], Loss: 1.1039\nEpoch [23/50], Step [70/75], Loss: 1.0498\nEpoch [23/50], Average Loss: 1.0283\nBLEU-1: 0.1993, BLEU-2: 0.1384, BLEU-3: 0.0968, BLEU-4: 0.0548\nEpoch [24/50], Step [10/75], Loss: 0.9060\nEpoch [24/50], Step [20/75], Loss: 1.0186\nEpoch [24/50], Step [30/75], Loss: 0.9517\nEpoch [24/50], Step [40/75], Loss: 1.0896\nEpoch [24/50], Step [50/75], Loss: 0.9891\nEpoch [24/50], Step [60/75], Loss: 1.1177\nEpoch [24/50], Step [70/75], Loss: 1.1429\nEpoch [24/50], Average Loss: 1.0129\nBLEU-1: 0.2010, BLEU-2: 0.1394, BLEU-3: 0.0994, BLEU-4: 0.0578\nEpoch [25/50], Step [10/75], Loss: 0.8405\nEpoch [25/50], Step [20/75], Loss: 0.9605\nEpoch [25/50], Step [30/75], Loss: 0.9747\nEpoch [25/50], Step [40/75], Loss: 1.0553\nEpoch [25/50], Step [50/75], Loss: 1.1155\nEpoch [25/50], Step [60/75], Loss: 1.0629\nEpoch [25/50], Step [70/75], Loss: 1.0978\nEpoch [25/50], Average Loss: 1.0033\nBLEU-1: 0.2084, BLEU-2: 0.1461, BLEU-3: 0.1046, BLEU-4: 0.0615\nEpoch [26/50], Step [10/75], Loss: 0.9078\nEpoch [26/50], Step [20/75], Loss: 0.9399\nEpoch [26/50], Step [30/75], Loss: 0.9114\nEpoch [26/50], Step [40/75], Loss: 1.0637\nEpoch [26/50], Step [50/75], Loss: 1.0275\nEpoch [26/50], Step [60/75], Loss: 1.1172\nEpoch [26/50], Step [70/75], Loss: 1.0765\nEpoch [26/50], Average Loss: 0.9921\nBLEU-1: 0.1971, BLEU-2: 0.1398, BLEU-3: 0.0990, BLEU-4: 0.0555\nEpoch [27/50], Step [10/75], Loss: 1.0105\nEpoch [27/50], Step [20/75], Loss: 0.8268\nEpoch [27/50], Step [30/75], Loss: 0.9005\nEpoch [27/50], Step [40/75], Loss: 0.9577\nEpoch [27/50], Step [50/75], Loss: 0.9903\nEpoch [27/50], Step [60/75], Loss: 1.0538\nEpoch [27/50], Step [70/75], Loss: 1.1029\nEpoch [27/50], Average Loss: 0.9749\nBLEU-1: 0.1976, BLEU-2: 0.1359, BLEU-3: 0.0936, BLEU-4: 0.0518\nEpoch [28/50], Step [10/75], Loss: 0.8762\nEpoch [28/50], Step [20/75], Loss: 0.8659\nEpoch [28/50], Step [30/75], Loss: 1.0323\nEpoch [28/50], Step [40/75], Loss: 0.9526\nEpoch [28/50], Step [50/75], Loss: 0.9699\nEpoch [28/50], Step [60/75], Loss: 1.1033\nEpoch [28/50], Step [70/75], Loss: 1.0345\nEpoch [28/50], Average Loss: 0.9630\nBLEU-1: 0.1954, BLEU-2: 0.1386, BLEU-3: 0.0992, BLEU-4: 0.0574\nEpoch [29/50], Step [10/75], Loss: 0.9335\nEpoch [29/50], Step [20/75], Loss: 0.9509\nEpoch [29/50], Step [30/75], Loss: 0.9490\nEpoch [29/50], Step [40/75], Loss: 1.0527\nEpoch [29/50], Step [50/75], Loss: 0.9807\nEpoch [29/50], Step [60/75], Loss: 1.0886\nEpoch [29/50], Step [70/75], Loss: 1.1340\nEpoch [29/50], Average Loss: 0.9549\nBLEU-1: 0.1965, BLEU-2: 0.1376, BLEU-3: 0.0954, BLEU-4: 0.0504\nEpoch [30/50], Step [10/75], Loss: 0.9780\nEpoch [30/50], Step [20/75], Loss: 0.9162\nEpoch [30/50], Step [30/75], Loss: 1.0149\nEpoch [30/50], Step [40/75], Loss: 1.0112\nEpoch [30/50], Step [50/75], Loss: 0.9404\nEpoch [30/50], Step [60/75], Loss: 0.9746\nEpoch [30/50], Step [70/75], Loss: 0.9787\nEpoch [30/50], Average Loss: 0.9383\nBLEU-1: 0.1936, BLEU-2: 0.1382, BLEU-3: 0.0991, BLEU-4: 0.0559\nEpoch [31/50], Step [10/75], Loss: 0.8042\nEpoch [31/50], Step [20/75], Loss: 0.9406\nEpoch [31/50], Step [30/75], Loss: 0.8476\nEpoch [31/50], Step [40/75], Loss: 1.0281\nEpoch [31/50], Step [50/75], Loss: 0.9832\nEpoch [31/50], Step [60/75], Loss: 0.9785\nEpoch [31/50], Step [70/75], Loss: 0.9601\nEpoch [31/50], Average Loss: 0.9322\nBLEU-1: 0.2134, BLEU-2: 0.1488, BLEU-3: 0.1066, BLEU-4: 0.0638\nEpoch [32/50], Step [10/75], Loss: 0.8190\nEpoch [32/50], Step [20/75], Loss: 0.8782\nEpoch [32/50], Step [30/75], Loss: 0.8797\nEpoch [32/50], Step [40/75], Loss: 0.9344\nEpoch [32/50], Step [50/75], Loss: 0.8981\nEpoch [32/50], Step [60/75], Loss: 1.0433\nEpoch [32/50], Step [70/75], Loss: 0.9416\nEpoch [32/50], Average Loss: 0.9205\nBLEU-1: 0.1916, BLEU-2: 0.1355, BLEU-3: 0.0957, BLEU-4: 0.0527\nEpoch [33/50], Step [10/75], Loss: 0.8738\nEpoch [33/50], Step [20/75], Loss: 0.8696\nEpoch [33/50], Step [30/75], Loss: 0.8287\nEpoch [33/50], Step [40/75], Loss: 0.9389\nEpoch [33/50], Step [50/75], Loss: 1.0342\nEpoch [33/50], Step [60/75], Loss: 0.9163\nEpoch [33/50], Step [70/75], Loss: 0.8926\nEpoch [33/50], Average Loss: 0.9058\nBLEU-1: 0.1890, BLEU-2: 0.1334, BLEU-3: 0.0952, BLEU-4: 0.0558\nEpoch [34/50], Step [10/75], Loss: 0.9117\nEpoch [34/50], Step [20/75], Loss: 0.7727\nEpoch [34/50], Step [30/75], Loss: 0.8455\nEpoch [34/50], Step [40/75], Loss: 0.9480\nEpoch [34/50], Step [50/75], Loss: 0.8929\nEpoch [34/50], Step [60/75], Loss: 0.9491\nEpoch [34/50], Step [70/75], Loss: 0.8377\nEpoch [34/50], Average Loss: 0.8951\nBLEU-1: 0.2047, BLEU-2: 0.1459, BLEU-3: 0.1054, BLEU-4: 0.0622\nEpoch [35/50], Step [10/75], Loss: 0.8136\nEpoch [35/50], Step [20/75], Loss: 0.7682\nEpoch [35/50], Step [30/75], Loss: 1.0067\nEpoch [35/50], Step [40/75], Loss: 0.9187\nEpoch [35/50], Step [50/75], Loss: 0.8474\nEpoch [35/50], Step [60/75], Loss: 0.8316\nEpoch [35/50], Step [70/75], Loss: 0.9299\nEpoch [35/50], Average Loss: 0.8909\nBLEU-1: 0.1994, BLEU-2: 0.1386, BLEU-3: 0.0973, BLEU-4: 0.0558\nEpoch [36/50], Step [10/75], Loss: 0.7744\nEpoch [36/50], Step [20/75], Loss: 0.8481\nEpoch [36/50], Step [30/75], Loss: 0.8927\nEpoch [36/50], Step [40/75], Loss: 0.8460\nEpoch [36/50], Step [50/75], Loss: 0.9535\nEpoch [36/50], Step [60/75], Loss: 1.0200\nEpoch [36/50], Step [70/75], Loss: 0.9058\nEpoch [36/50], Average Loss: 0.8699\nBLEU-1: 0.2039, BLEU-2: 0.1426, BLEU-3: 0.0997, BLEU-4: 0.0587\nEpoch [37/50], Step [10/75], Loss: 0.7357\nEpoch [37/50], Step [20/75], Loss: 0.8683\nEpoch [37/50], Step [30/75], Loss: 0.8302\nEpoch [37/50], Step [40/75], Loss: 0.8523\nEpoch [37/50], Step [50/75], Loss: 1.0220\nEpoch [37/50], Step [60/75], Loss: 0.9438\nEpoch [37/50], Step [70/75], Loss: 0.9150\nEpoch [37/50], Average Loss: 0.8656\nBLEU-1: 0.1937, BLEU-2: 0.1351, BLEU-3: 0.0944, BLEU-4: 0.0528\nEpoch [38/50], Step [10/75], Loss: 0.7564\nEpoch [38/50], Step [20/75], Loss: 0.9083\nEpoch [38/50], Step [30/75], Loss: 0.8096\nEpoch [38/50], Step [40/75], Loss: 0.8195\nEpoch [38/50], Step [50/75], Loss: 0.8312\nEpoch [38/50], Step [60/75], Loss: 0.9424\nEpoch [38/50], Step [70/75], Loss: 1.0165\nEpoch [38/50], Average Loss: 0.8570\nBLEU-1: 0.2087, BLEU-2: 0.1489, BLEU-3: 0.1084, BLEU-4: 0.0665\nEpoch [39/50], Step [10/75], Loss: 0.8100\nEpoch [39/50], Step [20/75], Loss: 0.7494\nEpoch [39/50], Step [30/75], Loss: 0.7717\nEpoch [39/50], Step [40/75], Loss: 0.8866\nEpoch [39/50], Step [50/75], Loss: 0.8547\nEpoch [39/50], Step [60/75], Loss: 0.8383\nEpoch [39/50], Step [70/75], Loss: 0.8601\nEpoch [39/50], Average Loss: 0.8503\nBLEU-1: 0.2021, BLEU-2: 0.1432, BLEU-3: 0.1019, BLEU-4: 0.0583\nEpoch [40/50], Step [10/75], Loss: 0.8134\nEpoch [40/50], Step [20/75], Loss: 0.7924\nEpoch [40/50], Step [30/75], Loss: 0.8774\nEpoch [40/50], Step [40/75], Loss: 0.8110\nEpoch [40/50], Step [50/75], Loss: 0.7819\nEpoch [40/50], Step [60/75], Loss: 0.9533\nEpoch [40/50], Step [70/75], Loss: 1.0421\nEpoch [40/50], Average Loss: 0.8475\nBLEU-1: 0.2072, BLEU-2: 0.1486, BLEU-3: 0.1076, BLEU-4: 0.0651\nEpoch [41/50], Step [10/75], Loss: 0.7400\nEpoch [41/50], Step [20/75], Loss: 0.8131\nEpoch [41/50], Step [30/75], Loss: 0.8436\nEpoch [41/50], Step [40/75], Loss: 0.8971\nEpoch [41/50], Step [50/75], Loss: 0.9309\nEpoch [41/50], Step [60/75], Loss: 0.9100\nEpoch [41/50], Step [70/75], Loss: 0.9295\nEpoch [41/50], Average Loss: 0.8289\nBLEU-1: 0.1987, BLEU-2: 0.1386, BLEU-3: 0.0989, BLEU-4: 0.0576\nEpoch [42/50], Step [10/75], Loss: 0.7282\nEpoch [42/50], Step [20/75], Loss: 0.7850\nEpoch [42/50], Step [30/75], Loss: 0.8930\nEpoch [42/50], Step [40/75], Loss: 0.9448\nEpoch [42/50], Step [50/75], Loss: 0.8863\nEpoch [42/50], Step [60/75], Loss: 0.8460\nEpoch [42/50], Step [70/75], Loss: 0.8484\nEpoch [42/50], Average Loss: 0.8420\nBLEU-1: 0.2007, BLEU-2: 0.1421, BLEU-3: 0.1008, BLEU-4: 0.0574\nEpoch [43/50], Step [10/75], Loss: 0.7603\nEpoch [43/50], Step [20/75], Loss: 0.7727\nEpoch [43/50], Step [30/75], Loss: 0.8943\nEpoch [43/50], Step [40/75], Loss: 0.6731\nEpoch [43/50], Step [50/75], Loss: 0.7737\nEpoch [43/50], Step [60/75], Loss: 0.7729\nEpoch [43/50], Step [70/75], Loss: 0.8756\nEpoch [43/50], Average Loss: 0.8253\nBLEU-1: 0.1952, BLEU-2: 0.1364, BLEU-3: 0.0964, BLEU-4: 0.0551\nEpoch [44/50], Step [10/75], Loss: 0.6584\nEpoch [44/50], Step [20/75], Loss: 0.7562\nEpoch [44/50], Step [30/75], Loss: 0.8270\nEpoch [44/50], Step [40/75], Loss: 0.8442\nEpoch [44/50], Step [50/75], Loss: 0.8435\nEpoch [44/50], Step [60/75], Loss: 0.9231\nEpoch [44/50], Step [70/75], Loss: 0.8722\nEpoch [44/50], Average Loss: 0.8312\nBLEU-1: 0.2011, BLEU-2: 0.1408, BLEU-3: 0.1002, BLEU-4: 0.0581\nEpoch [45/50], Step [10/75], Loss: 0.7154\nEpoch [45/50], Step [20/75], Loss: 0.7863\nEpoch [45/50], Step [30/75], Loss: 0.7795\nEpoch [45/50], Step [40/75], Loss: 0.7775\nEpoch [45/50], Step [50/75], Loss: 0.8300\nEpoch [45/50], Step [60/75], Loss: 0.8146\nEpoch [45/50], Step [70/75], Loss: 0.9753\nEpoch [45/50], Average Loss: 0.8118\nBLEU-1: 0.1941, BLEU-2: 0.1371, BLEU-3: 0.0979, BLEU-4: 0.0563\nEpoch [46/50], Step [10/75], Loss: 0.7265\nEpoch [46/50], Step [20/75], Loss: 0.6953\nEpoch [46/50], Step [30/75], Loss: 0.7249\nEpoch [46/50], Step [40/75], Loss: 0.8286\nEpoch [46/50], Step [50/75], Loss: 0.8110\nEpoch [46/50], Step [60/75], Loss: 0.8522\nEpoch [46/50], Step [70/75], Loss: 0.8321\nEpoch [46/50], Average Loss: 0.7994\nBLEU-1: 0.1929, BLEU-2: 0.1334, BLEU-3: 0.0921, BLEU-4: 0.0481\nEpoch [47/50], Step [10/75], Loss: 0.8251\nEpoch [47/50], Step [20/75], Loss: 0.8226\nEpoch [47/50], Step [30/75], Loss: 0.8035\nEpoch [47/50], Step [40/75], Loss: 0.8335\nEpoch [47/50], Step [50/75], Loss: 0.7846\nEpoch [47/50], Step [60/75], Loss: 0.9072\nEpoch [47/50], Step [70/75], Loss: 0.9253\nEpoch [47/50], Average Loss: 0.8012\nBLEU-1: 0.2026, BLEU-2: 0.1435, BLEU-3: 0.1035, BLEU-4: 0.0616\nEpoch [48/50], Step [10/75], Loss: 0.8058\nEpoch [48/50], Step [20/75], Loss: 0.7489\nEpoch [48/50], Step [30/75], Loss: 0.8047\nEpoch [48/50], Step [40/75], Loss: 0.7916\nEpoch [48/50], Step [50/75], Loss: 0.7507\nEpoch [48/50], Step [60/75], Loss: 0.7563\nEpoch [48/50], Step [70/75], Loss: 0.8528\nEpoch [48/50], Average Loss: 0.7904\nBLEU-1: 0.2072, BLEU-2: 0.1463, BLEU-3: 0.1050, BLEU-4: 0.0623\nEpoch [49/50], Step [10/75], Loss: 0.7112\nEpoch [49/50], Step [20/75], Loss: 0.6126\nEpoch [49/50], Step [30/75], Loss: 0.7532\nEpoch [49/50], Step [40/75], Loss: 0.8323\nEpoch [49/50], Step [50/75], Loss: 0.8287\nEpoch [49/50], Step [60/75], Loss: 0.8292\nEpoch [49/50], Step [70/75], Loss: 0.7861\nEpoch [49/50], Average Loss: 0.7862\nBLEU-1: 0.2082, BLEU-2: 0.1488, BLEU-3: 0.1073, BLEU-4: 0.0645\nEpoch [50/50], Step [10/75], Loss: 0.7098\nEpoch [50/50], Step [20/75], Loss: 0.7267\nEpoch [50/50], Step [30/75], Loss: 0.7611\nEpoch [50/50], Step [40/75], Loss: 0.7719\nEpoch [50/50], Step [50/75], Loss: 0.7646\nEpoch [50/50], Step [60/75], Loss: 0.8911\nEpoch [50/50], Step [70/75], Loss: 0.9300\nEpoch [50/50], Average Loss: 0.7761\nBLEU-1: 0.2069, BLEU-2: 0.1493, BLEU-3: 0.1094, BLEU-4: 0.0680\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 3-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 4-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \nThe hypothesis contains 0 counts of 2-gram overlaps.\nTherefore the BLEU score evaluates to 0, independently of\nhow many N-gram overlaps of lower order it contains.\nConsider using lower n-gram order or use SmoothingFunction()\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"\n=== Examples with Highest BLEU-4 Scores ===\nBLEU-4 Score: 1.0000\nReference: crystal structure of mutant human lysozyme substituted at the surface\nGenerated: crystal structure of mutant human lysozyme substituted at the surface\n---\nBLEU-4 Score: 0.7071\nReference: crystal structure of the omtky3 p1 variant omtky3-val18i in complex\nGenerated: crystal structure of the omtky3 p1 variant omtky3-thr18i in complex\n---\nBLEU-4 Score: 0.6804\nReference: crystal structure of platelet factor 4 mutant 2\nGenerated: crystal structure of platelet factor 4 in the\n---\nBLEU-4 Score: 0.6004\nReference: structure of carbamoyl phosphate synthetase complexed with the atp\nGenerated: crystal structure of carbamoyl phosphate synthetase complexed at\n---\nBLEU-4 Score: 0.5877\nReference: t4 lysozyme mutant c54t/c97a/l121a in the presence of 8 atm xenon\nGenerated: t4 lysozyme mutant c54t/c97a/l133a in the presence of 8 atm argon\n---\n\n=== Examples with Lowest BLEU-4 Scores ===\nBLEU-4 Score: 0.0000\nReference: fusarium oxysporum trypsin at atomic resolution\nGenerated: crystal structure of t4 lysozyme mutant t152s in the presence of 8 atm argon\n---\nBLEU-4 Score: 0.0000\nReference: apo ovotransferrin\nGenerated: crystal structure of the plasmid maintenance system epsilon/zeta:\n---\nBLEU-4 Score: 0.0000\nReference: cytochrome c' from rhodobacter spheriodes\nGenerated: crystal structure of the xylanase cex with xylobiose-derived lactam\n---\nBLEU-4 Score: 0.0000\nReference: filamentous bacteriophage ph75\nGenerated: crystal structure of a berenil-d(cgcaaatttgcg) complex; an example of\n---\nBLEU-4 Score: 0.0000\nReference: plasminogen activator (tsv-pa) from snake venom\nGenerated: crystal structure of a berenil-d(cgcaaatttgcg) complex; an example of\n---\n\nAverage BLEU-4 Score: 0.0249\n\nGenerated Caption (Example): solution structure of the c-terminal domain of human rpa32 complexed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":8}]}